# 🧠 Workshop 3: Machine Learning & Data Streaming – Happiness Score Prediction  
**Developed by:**  
Luis Angel Garcia (2230177)

This project implements a complete Machine Learning pipeline combined with real-time data streaming using Kafka and PostgreSQL.  
A regression model is trained to predict the **Happiness Score** of countries (based on World Happiness Reports from 2015–2019).  
The system simulates real-time ingestion, prediction, and storage using a Kafka-based architecture.

---

## 📁 Project Structure

```
workshop_003/
│
├── data/                    # Raw + cleaned data
│   ├── 2015.csv … 2019.csv  # Original datasets
│   └── happiness_cleaned.csv  # Cleaned dataset (generated by notebook)
│
├── database/                # DB init and query script
│   ├── init.sql             # Creates predictions table (used by Docker)
│   └── consultar_db.py      # Console query to PostgreSQL
│
├── kafka/                   # Streaming logic
│   ├── config_postgres.py   # PostgreSQL credentials
│   ├── producer.py          # Streams data
│   └── consumer.py          # Predicts + saves to PostgreSQL
│
├── models/                  # Trained model and scaler
│   ├── happiness_model.pkl
│   └── scaler.pkl
│
├── notebooks/
│   └── eda_etl.ipynb        # Data cleaning and model training
│
├── Dockerfile               # Container for running the pipeline
├── docker-compose.yml       # Orchestrates Kafka, PostgreSQL, App
├── requirements.txt         # Python dependencies
└── README.md
```

---

## ⚙️ Technologies Used

- Python + Jupyter Notebook
- Pandas, Scikit-learn, Joblib
- Apache Kafka + kafka-python
- PostgreSQL 
- Docker + Docker Compose

---

## 🔄 Project Flow

1. Clean and unify 5 CSVs from World Happiness Reports (2015–2019)
2. Train a regression model (Linear, Ridge, Huber)
3. Send rows to Kafka using a producer
4. Consume data → scale features → predict Happiness Score
5. Store predictions in a PostgreSQL database
6. Query results using a custom script

---

## 🧹 EDA and ETL

- Standardized columns across all years
- Dropped unused variables (`Region`, error margins)
- Imputed missing values (`Trust`, `Dystopia`)
- Unified into a clean dataset: `happiness_cleaned.csv`

---

## 🧠 Model Training

Three models were trained to compare performance:

- **Linear Regression**
- **Ridge Regression**
- **Huber Regressor**

### 📊 Metrics Comparison

| Metric | Linear | Ridge | Huber |
|--------|--------|-------|-------|
| R²     | 0.9863 | 0.9862| 0.9859|
| RMSE   | 0.1307 | 0.1313| 0.1328|
| MAE    | 0.0990 | 0.0986| 0.0938|
| MSE    | 0.0171 | 0.0172| 0.0176|

**Accuracy (±0.3 tolerance):**
- Linear Regression: 97.02%
- Ridge Regression: 97.02%
- Huber Regressor: 96.6%

➡️ **Final Decision:** Linear Regression selected for its strong performance and simplicity.

---

## 🔁 Kafka Streaming

### 📨 Producer (`kafka/producer.py`)

- Reads from `happiness_cleaned.csv`
- Sends rows to Kafka topic `happiness_topic`
- Excludes columns `Country` and `Score` (target)

### 🤖 Consumer (`kafka/consumer.py`)

- Listens to `happiness_topic`
- Applies `scaler.pkl` and predicts using `happiness_model.pkl`
- Saves result to PostgreSQL (`predictions` table)

---

## 💾 PostgreSQL Database

- Table: `predictions`
- Columns:
  - All input features (GDP, Social support, etc.)
  - Year
  - Predicted Happiness Score

### Query predictions

```bash
python database/consultar_db.py
```

---

## 📊 Model Evaluation

A scatter plot of **Predicted vs Actual** shows that the model closely follows the ideal line.  
No major outliers or bias were observed.

---

## 🚀 How to Run the Project

### 1. Clone the repository

```bash
git clone https://github.com/Luisgar0821/workshop_003
cd workshop_003
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  
```

### 3. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the notebook to generate model and cleaned dataset

```bash
jupyter notebook notebooks/eda_etl.ipynb
```

This will create:

- `data/happiness_cleaned.csv`
- `models/happiness_model.pkl`
- `models/scaler.pkl`

### 5. Start Docker services (Kafka + PostgreSQL)

```bash
docker-compose up -d
```

### 6. Run the pipeline

In one terminal:

```bash
python kafka/consumer.py
```

In another terminal:

```bash
python kafka/producer.py
```

### 7. View predictions in the database

```bash
python database/consultar_db.py
```

---

## 📦 Requirements (requirements.txt)

```txt
pandas
scikit-learn
joblib
kafka-python
psycopg2
```

---