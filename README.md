# ğŸ§  Workshop 3: Machine Learning & Data Streaming â€“ Happiness Score Prediction  
**Developed by:**  
Luis Angel Garcia (2230177)

This project implements a complete Machine Learning pipeline combined with real-time data streaming using Kafka and PostgreSQL.  
A regression model is trained to predict the **Happiness Score** of countries (based on World Happiness Reports from 2015â€“2019).  
The system simulates real-time ingestion, prediction, and storage using a Kafka-based architecture.

---

## ğŸ“ Project Structure

```
workshop_003/
â”‚
â”œâ”€â”€ data/                    # Raw + cleaned data
â”‚   â”œâ”€â”€ 2015.csv â€¦ 2019.csv  # Original datasets
â”‚   â””â”€â”€ happiness_cleaned.csv  # Cleaned dataset (generated by notebook)
â”‚
â”œâ”€â”€ database/                # DB init and query script
â”‚   â”œâ”€â”€ init.sql             # Creates predictions table (used by Docker)
â”‚   â””â”€â”€ consultar_db.py      # Console query to PostgreSQL
â”‚
â”œâ”€â”€ kafka/                   # Streaming logic
â”‚   â”œâ”€â”€ config_postgres.py   # PostgreSQL credentials
â”‚   â”œâ”€â”€ producer.py          # Streams data
â”‚   â””â”€â”€ consumer.py          # Predicts + saves to PostgreSQL
â”‚
â”œâ”€â”€ models/                  # Trained model and scaler
â”‚   â”œâ”€â”€ happiness_model.pkl
â”‚   â””â”€â”€ scaler.pkl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_etl.ipynb        # Data cleaning and model training
â”‚
â”œâ”€â”€ Dockerfile               # Container for running the pipeline
â”œâ”€â”€ docker-compose.yml       # Orchestrates Kafka, PostgreSQL, App
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

- Python + Jupyter Notebook
- Pandas, Scikit-learn, Joblib
- Apache Kafka + kafka-python
- PostgreSQL 
- Docker + Docker Compose

---

## ğŸ”„ Project Flow

1. Clean and unify 5 CSVs from World Happiness Reports (2015â€“2019)
2. Train a regression model (Linear, Ridge, Huber)
3. Send rows to Kafka using a producer
4. Consume data â†’ scale features â†’ predict Happiness Score
5. Store predictions in a PostgreSQL database
6. Query results using a custom script

---

## ğŸ§¹ EDA and ETL

- Standardized columns across all years
- Dropped unused variables (`Region`, error margins)
- Imputed missing values (`Trust`, `Dystopia`)
- Unified into a clean dataset: `happiness_cleaned.csv`

---

## ğŸ§  Model Training

Three models were trained to compare performance:

- **Linear Regression**
- **Ridge Regression**
- **Huber Regressor**

### ğŸ“Š Metrics Comparison

| Metric | Linear | Ridge | Huber |
|--------|--------|-------|-------|
| RÂ²     | 0.9863 | 0.9862| 0.9859|
| RMSE   | 0.1307 | 0.1313| 0.1328|
| MAE    | 0.0990 | 0.0986| 0.0938|
| MSE    | 0.0171 | 0.0172| 0.0176|

**Accuracy (Â±0.3 tolerance):**
- Linear Regression: 97.02%
- Ridge Regression: 97.02%
- Huber Regressor: 96.6%

â¡ï¸ **Final Decision:** Linear Regression selected for its strong performance and simplicity.

---

## ğŸ” Kafka Streaming

### ğŸ“¨ Producer (`kafka/producer.py`)

- Reads from `happiness_cleaned.csv`
- Sends rows to Kafka topic `happiness_topic`
- Excludes columns `Country` and `Score` (target)

### ğŸ¤– Consumer (`kafka/consumer.py`)

- Listens to `happiness_topic`
- Applies `scaler.pkl` and predicts using `happiness_model.pkl`
- Saves result to PostgreSQL (`predictions` table)

---

## ğŸ’¾ PostgreSQL Database

- Table: `predictions`
- Columns:
  - All input features (GDP, Social support, etc.)
  - Year
  - Predicted Happiness Score

### Query predictions

```bash
python database/consultar_db.py
```

---

## ğŸ“Š Model Evaluation

A scatter plot of **Predicted vs Actual** shows that the model closely follows the ideal line.  
No major outliers or bias were observed.

---

## ğŸš€ How to Run the Project

### 1. Clone the repository

```bash
git clone https://github.com/Luisgar0821/workshop_003
cd workshop_003
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  
```

### 3. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the notebook to generate model and cleaned dataset

```bash
jupyter notebook notebooks/eda_etl.ipynb
```

This will create:

- `data/happiness_cleaned.csv`
- `models/happiness_model.pkl`
- `models/scaler.pkl`

### 5. Start Docker services (Kafka + PostgreSQL)

```bash
docker-compose up -d
```

### 6. Run the pipeline

In one terminal:

```bash
python kafka/consumer.py
```

In another terminal:

```bash
python kafka/producer.py
```

### 7. View predictions in the database

```bash
python database/consultar_db.py
```

---

## ğŸ“¦ Requirements (requirements.txt)

```txt
pandas
scikit-learn
joblib
kafka-python
psycopg2
```

---