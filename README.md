# ğŸ§  Workshop 3: Machine Learning & Data Streaming â€“ Happiness Score Prediction

This project implements a full Machine Learning pipeline combined with real-time data processing using Kafka. A regression model is trained to predict the Happiness Score of various countries using data from 2015 to 2019. A streaming system processes the data and stores predictions and features in a database.

---

## ğŸ“ Project Structure

```
workshop_003/
â”‚
â”œâ”€â”€ data/                    # Cleaned CSV
â”‚   â””â”€â”€ dataset_limpio.csv
â”‚
â”œâ”€â”€ database/                # SQLite DB and query script
â”‚   â”œâ”€â”€ predictions.db
â”‚   â””â”€â”€ consultar_db.py
â”‚
â”œâ”€â”€ kafka/                   # Kafka streaming scripts
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ consumer.py
â”‚
â”œâ”€â”€ models/                  # Trained model
â”‚   â”œâ”€â”€ happiness_model.pkl
â”‚   â””â”€â”€ scaler.pkl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_etl.ipynb        # EDA and training
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

* Python
* Jupyter Notebook
* Pandas, Scikit-learn, Joblib
* Kafka + kafka-python
* SQLite
* Docker + Docker Compose

---

## ğŸ”„ Project Flow

1. Load 5 CSV files with yearly happiness data.
2. Standardize and clean the data.
3. Train a regression model with 70-30 split.
4. Stream the cleaned dataset using Kafka (producer).
5. Consume the data, predict scores in real-time.
6. Store features and predictions in a database.

---

## ğŸ§¹ EDA and ETL

* Detected differences in columns between years.
* Renamed and standardized columns (`GDP`, `Health`, etc.).
* Dropped unnecessary columns (`Region`, `Standard Error`).
* Imputed missing values (`Trust`, `Dystopia`).

---

## ğŸ§  Model Training

* Model: `LinearRegression`
* Features used:

  * `GDP`, `Social support`, `Health`, `Freedom`, `Trust`, `Generosity`, `Dystopia`, `Year`
* Metrics:

  * ğŸ“ˆ **RÂ² Score**: 0.9863
  * ğŸ“ **RMSE**: 0.1307
  * ğŸ“‰ **MSE**: 0.0171
  * ğŸ“Š **MAE**: 0.0990

---

## ğŸ” Kafka Streaming

### Producer (`kafka/producer.py`)

* Reads from `dataset_limpio.csv`
* Sends each row to Kafka topic `happiness_topic` (excluding `Country` and `Score`)

### Consumer (`kafka/consumer.py`)

* Receives data from Kafka
* Applies `scaler.pkl`, predicts with `happiness_model.pkl`
* Inserts results into `predictions.db`

---

## ğŸ’¾ SQLite Database

* Database: `predictions.db`
* Table: `predictions`
* Stores:

  * Input features
  * Year
  * Predicted `Happiness Score`

### Query results

Run:

```bash
python database/consultar_db.py
```

---

## ğŸ“Š Model Evaluation

Generated plot: Predicted vs Actual
Conclusion: The predictions closely follow the ideal line. No major error trends or outliers.

---

## ğŸš€ How to Run the Project

### 1. Clone the repository

```bash
git clone <repo-url>
cd workshop_003
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Start Kafka

```bash
cd kafka
docker-compose up -d
```

### 5. Run producer and consumer

In one terminal:

```bash
python kafka/consumer.py
```

In another terminal:

```bash
python kafka/producer.py
```

### 6. View predictions

```bash
python database/consultar_db.py
```

---

## ğŸ“¦ Requirements (requirements.txt)

```txt
pandas
scikit-learn
joblib
kafka-python
```

---

## Project Status: **COMPLETE AND FUNCTIONAL**
